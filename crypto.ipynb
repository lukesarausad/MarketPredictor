{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetches the articles using a NYT API and obtains the title, abstract, lead_paragraph, keywords and publication date and puts it into nyt_articles_details.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved article details to nyt_articles_details.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "url = 'https://api.nytimes.com/svc/archive/v1/2024/1.json?api-key=a2tKA0T1H3RhXAqw4VlTaSJq5BqejQ7g'\n",
    "\n",
    "API_KEY = 'a2tKA0T1H3RhXAqw4VlTaSJq5BqejQ7g'\n",
    "\n",
    "def fetch_articles(month):\n",
    "    start_date = month.strftime('%Y%m01')\n",
    "    end_date = (month + timedelta(days=31)).strftime('%Y%m01')\n",
    "    params = {\n",
    "        'begin_date': start_date,\n",
    "        'end_date': end_date,\n",
    "        'api-key': API_KEY\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    return response.json()\n",
    "\n",
    "def extract_article_details(articles):\n",
    "    article_details = []\n",
    "    for article in articles['response']['docs']:\n",
    "        details = {\n",
    "            'title' : article.get('headline').get('main'),\n",
    "            'abstract': article.get('abstract'),\n",
    "            'lead_paragraph': article.get('lead_paragraph'),\n",
    "            'keywords': [keyword['value'] for keyword in article.get('keywords', [])],\n",
    "            'pub_date': article.get('pub_date')\n",
    "        }\n",
    "        article_details.append(details)\n",
    "    return article_details\n",
    "\n",
    "def save_articles_to_json(data, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "# Fetch articles for the past month\n",
    "current_month = datetime.now().replace(day=1)\n",
    "past_month = current_month - timedelta(days=30)\n",
    "data = fetch_articles(past_month)\n",
    "\n",
    "# Extract specific details\n",
    "article_details = extract_article_details(data)\n",
    "\n",
    "# Save the extracted details to a JSON file\n",
    "save_articles_to_json(article_details, 'nyt_articles_details.json')\n",
    "\n",
    "print(f'Saved article details to nyt_articles_details.json')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Searches for matches in each article of keywords in the title, abstract, lead_paragraph or keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14 articles matching the search terms.\n",
      "Saved matched articles to nyt_matched_articles.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def search_articles(articles, search_terms):\n",
    "    matched_articles = []\n",
    "    \n",
    "    for article in articles:\n",
    "        # Combine all searchable fields into a single string\n",
    "        searchable_text = (\n",
    "            (article.get('title') or '') + ' ' +\n",
    "            ' '.join(article.get('keywords') or []) + ' ' +\n",
    "            (article.get('abstract') or '') + ' ' +\n",
    "            (article.get('lead_paragraph') or '')\n",
    "        ).lower()  # Convert to lowercase for case-insensitive search\n",
    "\n",
    "        # Check if any of the search terms are in the searchable text\n",
    "        if any(term.lower() in searchable_text for term in search_terms):\n",
    "            matched_articles.append(article)\n",
    "\n",
    "    return matched_articles\n",
    "\n",
    "def save_articles_to_json(data, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "# Load the condensed JSON file with article details\n",
    "with open('nyt_articles_details.json', 'r') as f:\n",
    "    articles = json.load(f)\n",
    "\n",
    "# Define the search terms (e.g., \"economy\", \"COVID-19\", etc.)\n",
    "search_terms = ['cryptocurrency', 'bitcoin', 'ethereum', 'blockchain', 'metaverse', 'web3']\n",
    "\n",
    "# Search for articles that contain any of the search terms\n",
    "matched_articles = search_articles(articles, search_terms)\n",
    "\n",
    "# Save the matched articles to a new JSON file\n",
    "save_articles_to_json(matched_articles, 'nyt_matched_articles.json')\n",
    "\n",
    "print(f'Found {len(matched_articles)} articles matching the search terms.')\n",
    "print(f'Saved matched articles to nyt_matched_articles.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get sentiment analysis of each article and inputs that at bottom of each article in nyt_analyzed_articles.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment analysis complete. Results saved to nyt_analyzed_articles.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/lukesarausad/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Download VADER's lexicon\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Initialize VADER sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    # Get the sentiment scores\n",
    "    sentiment_scores = sia.polarity_scores(text)\n",
    "    compound_score = sentiment_scores['compound']\n",
    "    \n",
    "    # Determine sentiment based on the compound score\n",
    "    if compound_score >= 0.05:\n",
    "        return 'positive'\n",
    "    elif compound_score <= -0.05:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "def perform_sentiment_analysis(articles):\n",
    "    for article in articles:\n",
    "        # Combine the relevant text fields for sentiment analysis\n",
    "        text = (\n",
    "            (article.get('title') or '') + ' ' +\n",
    "            (article.get('abstract') or '') + ' ' +\n",
    "            (article.get('lead_paragraph') or '')\n",
    "        )\n",
    "        \n",
    "        # Perform sentiment analysis\n",
    "        sentiment = analyze_sentiment(text)\n",
    "        article['sentiment'] = sentiment\n",
    "\n",
    "    return articles\n",
    "\n",
    "def save_articles_to_json(data, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "# Load the matched articles JSON file\n",
    "with open('nyt_matched_articles.json', 'r') as f:\n",
    "    articles = json.load(f)\n",
    "\n",
    "# Perform sentiment analysis on the matched articles\n",
    "analyzed_articles = perform_sentiment_analysis(articles)\n",
    "\n",
    "# Save the analyzed articles with sentiment to a new JSON file\n",
    "save_articles_to_json(analyzed_articles, 'nyt_analyzed_articles.json')\n",
    "\n",
    "print(f'Sentiment analysis complete. Results saved to nyt_analyzed_articles.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculates overall sentiment analysis for all articles and if it positively, neutral or negatively impacts the price of given crypto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment suggests a potential positive impact on ETH.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to simulate impact based on sentiment analysis\n",
    "def predict_crypto_impact(sentiment_data, crypto_symbol):\n",
    "    sentiment_counts = {'positive': 0, 'negative': 0, 'neutral': 0}\n",
    "    \n",
    "    for article in sentiment_data:\n",
    "        sentiment_counts[article['sentiment']] += 1\n",
    "\n",
    "    total_articles = len(sentiment_data)\n",
    "    \n",
    "    if total_articles == 0:\n",
    "        return f\"No relevant articles found for {crypto_symbol}.\"\n",
    "\n",
    "    positive_ratio = sentiment_counts['positive'] / total_articles\n",
    "    negative_ratio = sentiment_counts['negative'] / total_articles\n",
    "\n",
    "    # Simple prediction logic based on sentiment ratio\n",
    "    if positive_ratio > negative_ratio:\n",
    "        prediction = f\"The sentiment suggests a potential positive impact on {crypto_symbol}.\"\n",
    "    elif negative_ratio > positive_ratio:\n",
    "        prediction = f\"The sentiment suggests a potential negative impact on {crypto_symbol}.\"\n",
    "    else:\n",
    "        prediction = f\"The sentiment suggests a neutral impact on {crypto_symbol}.\"\n",
    "\n",
    "    return prediction\n",
    "\n",
    "def load_analyzed_articles(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# User input for cryptocurrency symbol (e.g., BTC, ETH)\n",
    "crypto_symbol = input(\"Enter the cryptocurrency symbol (e.g., BTC, ETH): \").upper()\n",
    "\n",
    "# Load the analyzed articles with sentiment data\n",
    "sentiment_data = load_analyzed_articles('nyt_analyzed_articles.json')\n",
    "\n",
    "# Predict impact on the chosen cryptocurrency\n",
    "prediction = predict_crypto_impact(sentiment_data, crypto_symbol)\n",
    "\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted price for ETH on nan based on recent sentiment: $3031.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/var/folders/64/q8xywmkx3c5cn8_061h56pkc0000gn/T/ipykernel_97446/2146956049.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['date'] = pd.to_datetime(df['Date']).dt.date  # Extract the date only\n",
      "/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import json\n",
    "\n",
    "# Function to fetch historical data using yfinance\n",
    "def fetch_historical_data(crypto_symbol, period='1mo'):\n",
    "    # yfinance requires the symbol to have '-USD' at the end for cryptocurrencies\n",
    "    ticker = f\"{crypto_symbol}-USD\"\n",
    "    df = yf.download(ticker, period=period, interval=\"1d\")\n",
    "    \n",
    "    if df.empty:\n",
    "        print(f\"Error: No data found for {crypto_symbol}.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df['price'] = df['Close']  # Use the Close price as the 'price'\n",
    "    df = df[['price']]  # We only need the price column\n",
    "    df.reset_index(inplace=True)\n",
    "    df['date'] = pd.to_datetime(df['Date']).dt.date  # Extract the date only\n",
    "    df.set_index('date', inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load sentiment data from JSON file\n",
    "with open('nyt_analyzed_articles.json', 'r') as f:\n",
    "    sentiment_data = json.load(f)\n",
    "\n",
    "# Aggregate sentiment scores by date\n",
    "sentiment_scores = []\n",
    "for article in sentiment_data:\n",
    "    if 'pub_date' not in article:\n",
    "        continue  # Skip the article if 'pub_date' is missing\n",
    "\n",
    "    date = pd.to_datetime(article['pub_date']).date()\n",
    "    score = 1 if article['sentiment'] == 'positive' else -1 if article['sentiment'] == 'negative' else 0\n",
    "    sentiment_scores.append({'date': date, 'sentiment_score': score})\n",
    "\n",
    "sentiment_df = pd.DataFrame(sentiment_scores)\n",
    "sentiment_df = sentiment_df.groupby('date').mean().reset_index()\n",
    "\n",
    "# Ensure the 'date' column in sentiment_df is datetime type\n",
    "sentiment_df['date'] = pd.to_datetime(sentiment_df['date'])\n",
    "\n",
    "# User input for cryptocurrency symbol (e.g., BTC, ETH)\n",
    "crypto_symbol = input(\"Enter the cryptocurrency symbol (e.g., BTC, ETH): \").upper()\n",
    "\n",
    "# Fetch historical price data for the chosen cryptocurrency using yfinance\n",
    "price_df = fetch_historical_data(crypto_symbol, period='1mo')\n",
    "\n",
    "# Ensure the index of price_df is datetime and reset the index to keep date as a column\n",
    "if not price_df.empty:\n",
    "    price_df.index = pd.to_datetime(price_df.index)\n",
    "\n",
    "    # Merge price data with sentiment scores\n",
    "    data_df = price_df.merge(sentiment_df, how='left', left_index=True, right_on='date').fillna(0)\n",
    "\n",
    "    # Prepare features (X) and target (y)\n",
    "    X = data_df[['sentiment_score']]\n",
    "    y = data_df['price']\n",
    "\n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "    # Train Linear Regression model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict future price based on recent sentiment\n",
    "    recent_sentiment_score = X_test.iloc[-1]['sentiment_score']\n",
    "    predicted_price = model.predict([[recent_sentiment_score]])\n",
    "\n",
    "    # Get the corresponding date for the prediction\n",
    "    prediction_date = X_test.index[-1]  # This corresponds to the last date in X_test\n",
    "\n",
    "    print(f\"Predicted price for {crypto_symbol} on {prediction_date} based on recent sentiment: ${predicted_price[0]:.2f}\")\n",
    "else:\n",
    "    print(\"No data available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date  sentiment_score\n",
      "0  2024-01-09              1.0\n",
      "1  2024-01-10              1.0\n",
      "2  2024-01-11              1.0\n",
      "3  2024-01-19              1.0\n",
      "4  2024-01-23              1.0\n"
     ]
    }
   ],
   "source": [
    "with open('nyt_analyzed_articles.json', 'r') as f:\n",
    "    sentiment_data = json.load(f)\n",
    "\n",
    "# Simulate sentiment scoring (assuming one score per day)\n",
    "sentiment_scores = []\n",
    "for article in sentiment_data:\n",
    "    # Check if 'pub_date' exists\n",
    "    if 'pub_date' not in article:\n",
    "        continue  # Skip the article if 'pub_date' is missing\n",
    "\n",
    "    # Convert the publication date\n",
    "    date = pd.to_datetime(article['pub_date']).date()\n",
    "    score = 1 if article['sentiment'] == 'positive' else -1 if article['sentiment'] == 'negative' else 0\n",
    "    sentiment_scores.append({'date': date, 'sentiment_score': score})\n",
    "\n",
    "sentiment_df = pd.DataFrame(sentiment_scores)\n",
    "sentiment_df = sentiment_df.groupby('date').mean().reset_index()\n",
    "\n",
    "# Now you can proceed with the rest of the script\n",
    "print(sentiment_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
